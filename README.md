# LLM Deep Dive - Educational Repository

This project aims to provide a step-by-step implementation of the Transformer model introduced in the ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. The goal is to offer an educational journey into understanding Transformers, from building the foundational model to exploring optimizations for inference, training, and more.


## Repository Overview

The repository is structured to provide a gradual learning curve, starting with the baseline Transformer implementation. As the project evolves, we'll introduce:

- [x] Baseline Implementation
- [ ] Inference optimizations
- [ ] Training optimizations
- [ ] Advanced techniques to improve model performance

Whether you're a student, researcher, or practitioner, this repository is designed to help you understand the core concepts and progressively dive into more advanced areas of the Transformer architecture.

## What's Next?

We're just getting started! In future updates, we'll be adding:

- [ ] Mixed-precision training
- [ ] Attention optimizations (Fused QKV)
- [ ] Beam search and other decoding strategies
- [ ] Techniques to reduce memory consumption and improve speed during inference

## Contributions

Contributions are welcome! If you'd like to contribute to the project, feel free to fork the repository, open an issue, or submit a pull request. Let's build a comprehensive educational resource for understanding and mastering Transformers!

## References

```
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
```
